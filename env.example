# =============================================================================
# Contract Assistant Configuration
# =============================================================================
# Copy this file to .env and customize the values for your environment
# cp env.example .env

# =============================================================================
# Environment Configuration
# =============================================================================
# Set the application environment
# Options: development, staging, production
ENVIRONMENT=development  

# Enable debug mode (only recommended for development)
DEBUG=true
LANGCHAIN_DEBUG=true
LANGCHAIN_TRACING_V2=false
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=YOUR_API_KEY
LANGCHAIN_PROJECT=Contract Assistant

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Choose your LLM provider
# Options: openai, local
LLM_PROVIDER=local  

# =============================================================================
# OpenAI Configuration (when LLM_PROVIDER=openai)
# =============================================================================
# Your OpenAI API key (required for OpenAI provider)
OPENAI_API_KEY=your_openai_api_key_here

# OpenAI Models
OPENAI_LLM_MODEL=gpt-4o-mini
OPENAI_EMBEDDINGS_MODEL=text-embedding-3-small

# =============================================================================
# Local Services Configuration (when LLM_PROVIDER=local)
# =============================================================================
# Local LLM Model (Ollama)
LOCAL_LLM_MODEL=llama3.2:3b

# Local Embeddings Model (Text Embeddings Inference)
LOCAL_EMBEDDINGS_MODEL=intfloat/multilingual-e5-large-instruct
LOCAL_EMBEDDINGS_PROVIDER=openai

# Model Aliases for LiteLLM
LOCAL_LLM_ALIAS=gpt-local
LOCAL_EMBEDDINGS_ALIAS=local-embeddings
# =============================================================================
# Dynamic Chunking Configuration
# =============================================================================
# --- Token-based Chunking ---
# Set the max tokens for your embeddings model to enable token-based chunking.
# Chars-based fallbacks are used if this is not set.

# For local TEI with `multilingual-e5-large-instruct`
EMBEDDINGS_MAX_INPUT_TOKENS=512

# For OpenAI `text-embedding-3-small`
# EMBEDDINGS_MAX_INPUT_TOKENS=8192

# --- Overrides and Tuning ---
# You can override the calculated chunk size and overlap in tokens.
# CHUNK_SIZE_TOKENS=400
# CHUNK_OVERLAP_TOKENS=80

# You can also tune the fractions used to derive chunk size/overlap from the token limit.
CHUNK_SIZE_FRACTION_OF_EMBED_LIMIT=0.9
CHUNK_OVERLAP_FRACTION=0.10

# --- Character-based Fallbacks ---
# These values are used if token-based chunking is disabled.
CHUNK_FALLBACK_CHARS=1000
CHUNK_FALLBACK_OVERLAP_CHARS=200

# Estimated characters per token.
AVG_CHARS_PER_TOKEN=4
# =============================================================================
# Service Configuration
# =============================================================================
# Main Application
APP_HOST=0.0.0.0
APP_PORT=8501

# LiteLLM Proxy Service
LITELLM_HOST=litellm
LITELLM_PORT=4000

# Ollama Service
CUSTOM_LLM_PROVIDER=ollama
OLLAMA_HOST=ollama
OLLAMA_PORT=11434

# Text Embeddings Inference Service
TEI_HOST=tei
TEI_PORT=8504

# =============================================================================
# Vector Store Configuration
# =============================================================================
# Type of vector store to use
# Options: chroma, postgres, sqlite
VECTOR_STORE_TYPE=chroma 

# ChromaDB persistence directory (relative to project root)
CHROMA_PERSIST_DIR=data/chroma_data
OLLAMA_PERSIST_DIR=data/weights/ollama_data
TEI_PERSIST_DIR=data/weights/tei_data

# =============================================================================
# QA Behavior Configuration
# =============================================================================
# Enable entity-based quick answers (disabled by default)
# Since entities are shown in sidebar, most users prefer full RAG answers with context
ENABLE_ENTITY_ROUTING=false

# =============================================================================
# Docker Configuration
# =============================================================================
# Docker network name for services
DOCKER_NETWORK_NAME=contract-net

# =============================================================================
# Local Development Overrides
# =============================================================================
# Uncomment and modify these for local development outside Docker
# LITELLM_HOST=localhost
# OLLAMA_HOST=localhost
# TEI_HOST=localhost
# APP_HOST=localhost

# PaddleOCR device (cpu or gpu)
PADDLE_DEVICE=gpu
