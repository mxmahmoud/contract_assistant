# =============================================================================
# Contract Assistant Configuration
# =============================================================================
# Copy this file to .env and customize the values for your environment
# cp env.example .env

# =============================================================================
# Environment Configuration
# =============================================================================
# Set the application environment
ENVIRONMENT=development  # Options: development, staging, production

# Enable debug mode (only recommended for development)
DEBUG=true
LANGCHAIN_DEBUG=true
LANGCHAIN_TRACING_V2=false
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=YOUR_API_KEY
LANGCHAIN_PROJECT=Contract Assistant

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Choose your LLM provider
LLM_PROVIDER=local  # Options: openai, local

# =============================================================================
# OpenAI Configuration (when LLM_PROVIDER=openai)
# =============================================================================
# Your OpenAI API key (required for OpenAI provider)
OPENAI_API_KEY=your_openai_api_key_here

# OpenAI Models
OPENAI_LLM_MODEL=gpt-4o-mini
OPENAI_EMBEDDINGS_MODEL=text-embedding-3-small

# =============================================================================
# Local Services Configuration (when LLM_PROVIDER=local)
# =============================================================================
# Local LLM Model (Ollama)
LOCAL_LLM_MODEL=llama3.2:3b

# Local Embeddings Model (Text Embeddings Inference)
LOCAL_EMBEDDINGS_MODEL=intfloat/multilingual-e5-large-instruct
LOCAL_EMBEDDINGS_PROVIDER=openai

# Model Aliases for LiteLLM
LOCAL_LLM_ALIAS=gpt-local
LOCAL_EMBEDDINGS_ALIAS=local-embeddings

# =============================================================================
# Service Configuration
# =============================================================================
# Main Application
APP_HOST=0.0.0.0
APP_PORT=8501

# LiteLLM Proxy Service
LITELLM_HOST=litellm
LITELLM_PORT=4000

# Ollama Service
CUSTOM_LLM_PROVIDER=ollama
OLLAMA_HOST=ollama
OLLAMA_PORT=11434

# Text Embeddings Inference Service
TEI_HOST=tei
TEI_PORT=8504

# =============================================================================
# Vector Store Configuration
# =============================================================================
# Type of vector store to use
VECTOR_STORE_TYPE=chroma  # Options: chroma, postgres, sqlite

# ChromaDB persistence directory (relative to project root)
CHROMA_PERSIST_DIR=data/chroma_data
OLLAMA_PERSIST_DIR=data/weights/ollama_data
TEI_PERSIST_DIR=data/weights/tei_data

# =============================================================================
# QA Behavior Configuration
# =============================================================================
# Keywords that force full RAG search instead of quick entity lookup
# Use comma-separated values (e.g., "explain,detailed,context")
RAG_BYPASS_KEYWORDS=explain,detailed,context,search,find,why,how,describe,tell me about

# =============================================================================
# Docker Configuration
# =============================================================================
# Docker network name for services
DOCKER_NETWORK_NAME=contract-net

# =============================================================================
# Local Development Overrides
# =============================================================================
# Uncomment and modify these for local development outside Docker
# LITELLM_HOST=localhost
# OLLAMA_HOST=localhost
# TEI_HOST=localhost
# APP_HOST=localhost
